# VisionTransformerImplementation

This project is a **step-by-step Jupyter Notebook implementation** of the **Vision Transformer (ViT)**, built entirely from scratch using **PyTorch**.

The goal of this notebook is to demystify the Vision Transformer architecture â€” breaking it down into digestible components while offering hands-on code examples, inline explanations, and real-world training with the **Oxford-IIIT Pet Dataset**.



## ðŸ“˜ Overview

The notebook walks through:
- âœ… Understanding patch embeddings via `Conv2D` projection.
- âœ… Implementing **Multi-Head Self Attention** manually.
- âœ… Building a custom **MLP Feedforward Block**.
- âœ… Composing the **Transformer Encoder**.
- âœ… Assembling the full **Vision Transformer Model**.
- âœ… Training and validating the model on the **Oxford-IIIT Pet Dataset**.

## ðŸ’¾ Dataset

This project uses:

- ðŸ“¦ **Oxford-IIIT Pet Dataset**  
A dataset of pet images annotated with class labels and pixel-level segmentation masks.

Official link: [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)

---
